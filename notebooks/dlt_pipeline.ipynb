{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a626959-61c8-4bba-84d2-2a4ecab1f7ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Lakeflow Pipeline\n",
    "\n",
    "This Declarative Lakeflow Pipeline uses Auto Loader to incrementally ingest Databricks cluster logs into tables for log analysis. This notebook is executed using a pipeline defined in resources/watchtower.pipeline.yml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9198e987-5606-403d-9f6d-8f14e6a4017f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "raw_log_location = spark.conf.get(\"raw_log_location\").rstrip(\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fc19dba-61fd-4a89-8f8c-24fee63bfb14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"bronze.logs\",\n",
    "    table_properties={\n",
    "        \"Quality\": \"bronze\",\n",
    "    },\n",
    ")\n",
    "@dlt.expect(\"valid_file_size\", \"file_size > 0\")\n",
    "@dlt.expect(\"non_empty_message\", \"msg IS NOT NULL\")\n",
    "def bronze_logs():\n",
    "    return (\n",
    "        spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"text\")\n",
    "            .load(f\"{raw_log_location}/*/driver/\")\n",
    "            .selectExpr(\n",
    "                \"value as msg\",\n",
    "                \"_metadata.file_path\",\n",
    "                \"_metadata.file_name\",\n",
    "                \"_metadata.file_size\",\n",
    "                \"_metadata.file_modification_time\"\n",
    "            )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a21a9f1-a100-457f-a457-6fa70bc7a3a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"silver.logs\",\n",
    "    table_properties={\n",
    "        \"Quality\": \"silver\",\n",
    "    },\n",
    "    cluster_by=[\"cluster_id\", \"log_ts\"]\n",
    ")\n",
    "@dlt.expect(\"non_empty_cluster_id\", \"cluster_id IS NOT NULL AND cluster_id != ''\")\n",
    "@dlt.expect(\"non_empty_log_source\", \"log_source IS NOT NULL\")\n",
    "@dlt.expect(\"parsed_log4j_log_level\", \"log_source != 'log4j' OR log_level != ''\")\n",
    "@dlt.expect(\"non_empty_log4j_ts\", \"log_source != 'log4j' OR log_ts IS NOT NULL\")\n",
    "def silver_logs():\n",
    "    return (\n",
    "        dlt.read_stream(\"bronze.logs\")\n",
    "            .withColumn(\"parsed_msg\", F.try_parse_json(\"msg\"))\n",
    "            .selectExpr(\n",
    "            # Parsing cluster_id and log_source which are part of the structured file path.\n",
    "            \"regexp_extract(file_path, '/([0-9]{4}-[0-9]{6}-[0-9a-z]+)/', 1) as cluster_id\",\n",
    "            \"regexp_extract(file_name, '^(stdout|stderr|log4j).*$', 1) as log_source\",\n",
    "\n",
    "            # When msg was parsed as structured JSON, it becomes easy and efficient to parse using VARIANT,\n",
    "            # otherwise, regex can be used, which is inefficient and more prone to mistakes.\n",
    "            \"\"\"CASE\n",
    "                WHEN parsed_msg IS NOT NULL THEN try_variant_get(parsed_msg, '$.level', 'string')\n",
    "                ELSE regexp_extract(msg, '\\\\\\\\b(TRACE|DEBUG|INFO|WARN|ERROR|FATAL|CRITICAL)\\\\\\\\b', 1)\n",
    "            END as log_level\"\"\",\n",
    "\n",
    "            \"\"\"CASE\n",
    "                WHEN parsed_msg IS NOT NULL THEN ifnull(\n",
    "                    try_variant_get(parsed_msg, '$.timestamp', 'timestamp'),\n",
    "                    try_variant_get(parsed_msg, '$.@timestamp', 'timestamp')\n",
    "                )\n",
    "                ELSE try_to_timestamp(\n",
    "                replace(\n",
    "                    regexp_extract(msg, '(\\\\\\\\d{2}[\\\\\\\\/\\-]\\\\\\\\d{2}[\\\\\\\\/\\\\\\\\-]\\\\\\\\d{2}\\\\\\\\s\\\\\\\\d{2}:\\\\\\\\d{2}:\\\\\\\\d{2})', 1),\n",
    "                    '/',\n",
    "                    '-'\n",
    "                ),\n",
    "                'yy-MM-dd HH:mm:ss'\n",
    "                ) END as log_ts\"\"\",\n",
    "\n",
    "            \"\"\"CASE\n",
    "                WHEN parsed_msg IS NOT NULL THEN try_variant_get(parsed_msg, '$.message', 'string')\n",
    "                ELSE msg\n",
    "            END as log_msg\"\"\",\n",
    "\n",
    "            \"file_path\",\n",
    "            \"file_name\",\n",
    "            \"file_size\",\n",
    "            \"file_modification_time\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c2e532a-650d-4cbb-937a-c97a00a5316e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"silver.job_run_compute\",\n",
    "    table_properties={\n",
    "        \"Quality\": \"silver\",\n",
    "    },\n",
    "    cluster_by=[\"compute_id\", \"job_id\"],\n",
    "    comment=\"Reference table of compute IDs used in each job run. Compute ID may be a cluster ID or warehouse ID.\"\n",
    ")\n",
    "@dlt.expect(\"non_empty_compute_id\", \"compute_id IS NOT NULL AND compute_id != ''\")\n",
    "@dlt.expect(\"non_empty_job_id\", \"job_id IS NOT NULL\")\n",
    "@dlt.expect(\"non_empty_workspace_id\", \"workspace_id IS NOT NULL\")\n",
    "@dlt.expect(\"non_empty_job_run_id\", \"job_run_id IS NOT NULL\")\n",
    "def job_run_compute():\n",
    "    return (spark.sql(\n",
    "      \"\"\"\n",
    "      SELECT DISTINCT workspace_id, job_id, job_run_id, explode(compute_ids) as compute_id\n",
    "      FROM system.lakeflow.job_task_run_timeline\n",
    "      WHERE\n",
    "        period_start_time >= NOW() - INTERVAL 7 DAYS\n",
    "    \"\"\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "dlt_pipeline",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
