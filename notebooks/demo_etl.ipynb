{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee353e42-ff58-4955-9608-12865bd0950e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Demo job\n",
    "\n",
    "This demo notebook is executed using Databricks Workflows as defined in resources/watchtower.demo.job.yml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up structured logging\n",
    "\n",
    "class JSONFormatter(logging.Formatter):\n",
    "    \"\"\"Structured JSON formatter for logging.\"\"\"\n",
    "\n",
    "    def format(self, record: logging.LogRecord) -> str:\n",
    "        \"\"\"Formats log records as JSON.\"\"\"\n",
    "        log_record = {\n",
    "            'timestamp': self.formatTime(record, self.datefmt),\n",
    "            'level': record.levelname,\n",
    "            'message': record.getMessage(),\n",
    "            'logger': record.name,\n",
    "            'line': record.lineno,\n",
    "        }\n",
    "        return json.dumps(log_record)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create a stream (console) handler\n",
    "# and set the formatter for the handler\n",
    "handler = logging.StreamHandler()\n",
    "formatter = JSONFormatter(datefmt='%Y-%m-%dT%H:%M:%S') # ISO-8601 format\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the logger object to log messages instead of print()\n",
    "logger.debug(\"This is a debug message\")\n",
    "logger.info(\"This is an info message\")\n",
    "logger.warning(\"This is a warning message\")\n",
    "\n",
    "# Handle exceptions with logging.\n",
    "try:\n",
    "    raise RuntimeError(\"This is a runtime error\")\n",
    "except RuntimeError:\n",
    "    logger.error(\"This is an error message\", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bca260b-13d1-448f-8082-30b60a85c9ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Intentionally setting a small executor memory\n",
    "# to demonstrate spill to disk.\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.executor.memory\", \"512m\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# FIXME: It is a best practice to remove .show() and display()\n",
    "#        from code before it is deployed to production.\n",
    "spark.range(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: we should not log PII or sensitive data.\n",
    "logger.info(\"user email: john.doe@example.com\")\n",
    "logger.info(\"phone: 5015551234\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "notebook_path = (\n",
    "    w.dbutils.notebook.entry_point\n",
    "        .getDbutils().notebook().getContext()\n",
    "        .notebookPath().get()\n",
    ")\n",
    "\n",
    "logger.info(f\"Notebook path: {notebook_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.streaming import StreamingQueryListener\n",
    "from pyspark.sql.streaming.listener import QueryStartedEvent, QueryProgressEvent, QueryTerminatedEvent\n",
    "\n",
    "\n",
    "class SparkStreamingLogger(StreamingQueryListener):\n",
    "    def onQueryStarted(self, event: QueryStartedEvent):\n",
    "        logger.info(f\"Query started: {event.name} ({event.id})\")\n",
    "\n",
    "    def onQueryProgress(self, event: QueryProgressEvent):\n",
    "        logger.info(f\"Query progress: {event.progress.json}\")\n",
    "\n",
    "    def onQueryTerminated(self, event: QueryTerminatedEvent):\n",
    "        if event.exception:\n",
    "            logger.error(f\"Query terminated: {event.id} ({event.exception})\")\n",
    "        else:\n",
    "            logger.info(f\"Query terminated: {event.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the logging listener\n",
    "spark.streams.addListener(SparkStreamingLogger())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
