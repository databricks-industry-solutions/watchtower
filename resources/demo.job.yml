# An example ETL job for watchtower to ingest logs/metrics from for demonstration.
resources:
  jobs:
    demo_workflow:
      name: watchtower_demo_job

      tasks:
        - task_key: demo_logger
          job_cluster_key: job_cluster
          notebook_task:
            notebook_path: ../tests/notebooks/demo_etl.py

        - task_key: refresh_watchtower
          pipeline_task:
            pipeline_id: ${resources.pipelines.watchtower_pipeline.id}
          depends_on:
            - task_key: demo_logger
          run_if: ALL_DONE

      job_clusters:
        - job_cluster_key: job_cluster
          new_cluster:
            spark_version: 15.4.x-scala2.12
            node_type_id: ${var.demo_node_type_id}
            data_security_mode: SINGLE_USER
            num_workers: 1

            # Configure your jobs to send logs to the watchtower volume
            cluster_log_conf:
              volumes:
                destination: ${var.cluster_logs_volume}

            # Logs from init scripts also generate logs into the watchtower volume.
            # In this case, we're using the init script to configure log4j
            # to get structured log formatting.
            init_scripts:
              - volumes:
                  destination: ${var.init_script_path}
