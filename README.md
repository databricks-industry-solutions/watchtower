# Watchtower - Databricks Logging Solution

A solution accelerator to standardizing, centralizing, and ingesting Databricks cluster logs
for enhanced log searching, troubleshooting, and optimization.

The goal of this repo is to provide platform adminsitrators and data engineers a reference implementation
for scalable log ingestion, as well as broader guidance for practioners to use logging in jobs.

![Log Search dashboard](./images/01-dashboard-logsearch.png)

![Top N analysis dashboard](./images/02-dashboard-top-n.png)

## Quick Start

1. **Prerequisites**
   ```bash
   pip install databricks-cli
   ```

2. **Configure Databricks**
   ```bash
   # Option A: Use databricks configure (interactive)
   databricks configure
   
   # Option B: Use environment file (recommended for CI/CD)
   cp env.example .env
   # Edit .env with your Databricks workspace URL, token, and warehouse ID
   # Get warehouse ID from: Databricks â†’ SQL Warehouses â†’ Copy warehouse ID
   ```

3. **Deploy Everything**
   ```bash
   ./scripts/deploy.sh
   ```

4. **Clean Up When Done**
   ```bash
   ./scripts/cleanup.sh
   ```

## CI/CD Setup (Optional)

To enable automatic testing on Pull Requests:

1. **Add GitHub Repository Secrets**:
   - Go to your repo â†’ Settings â†’ Secrets and variables â†’ Actions
   - Add secret: `DATABRICKS_TOKEN` (your Databricks token)
   - Optionally add variable: `DATABRICKS_HOST` (defaults to `https://e2-demo-field-eng.cloud.databricks.com/`)

2. **What happens automatically**:
   - **Pull Requests**: Validated and tested with isolated workspace paths
   - **Main branch**: Deployed to your dev environment
   - **PR cleanup**: Resources automatically cleaned up when PR is closed

## What Gets Deployed

- **Pipeline**: `Databricks log ingestion pipeline` 
- **Workflow**: `watchtower_demo_job`
- **Dashboard**: `Logs Dashboard` (deployed alongside pipeline)
- **Location**: `/Workspace/Users/your-email@company.com/.bundle/watchtower/`

## Manual Commands (if you prefer)

```bash
cd terraform && terraform init && terraform apply && cd .. # Deploy Catalog resources and init scripts
databricks bundle validate    # Check configuration
databricks bundle deploy      # Deploy to workspace
databricks bundle run demo_workflow # Run the demo workflow
databricks bundle summary     # See what's deployed
databricks bundle destroy     # Remove everything
cd terraform && terraform destroy && cd ..
```

## Customizing for Your Project

1. Update `databricks.yml` or the `resources/*.yml` files with your job/pipeline settings
2. Modify `notebooks/dlt_pipeline.ipynb` to customize the log ingestion pipeline
3. Modify the workspace `host` and `root_path` as needed
4. Modify `terraform/main.tf` as needed, or create a `terraform/.auto.tfvars` file to override Terraform variables.

## Project Structure

```
â”œâ”€â”€ databricks.yml           # Main DABs configuration
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ dlt_pipeline.ipynb   # Main watchtower log ingestion pipeline
â”‚   â””â”€â”€ demo_etl.ipynb       # Notebook for the demo job
â”œâ”€â”€ dashboards/
â”‚   â””â”€â”€ watchtower.lvdash.json    # Main watchtower dashboard for log analysis
â”œâ”€â”€ resources/                    # DAB resources to deploy
â”‚   â”œâ”€â”€ watchtower.dashboard.yml  # Main watchtower dashboard for log analysis
â”‚   â”œâ”€â”€ watchtower.demo.job.yml   # Demo job using structured logging frameworks
â”‚   â””â”€â”€ watchtower.pipeline.yml   # Main pipeline for ingesting logs
â”œâ”€â”€ terraform/
â”‚   â””â”€â”€ init-scripts/
â”‚       â””â”€â”€ configure_log4j.sh  # Modifies Spark driver log4j2.xml to use JSON format
â””â”€â”€ scripts/
    â”œâ”€â”€ deploy.sh           # Automated deployment wrapper
    â””â”€â”€ cleanup.sh          # Automated cleanup wrapper
```

That's it! ðŸš€ 